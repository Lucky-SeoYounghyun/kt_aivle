{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPLaV3dCS22+3nPYOXtuV94"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fneblP1UA-NB","executionInfo":{"status":"ok","timestamp":1730942893791,"user_tz":-540,"elapsed":173243,"user":{"displayName":"서영현","userId":"09271535808037779652"}},"outputId":"c8f18b8f-7262-4dee-b29e-8514ac841881"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Iteration] 0 [cost] 1.0741775035858154\n","[Iteration] 10000 [cost] 0.00746897840872407\n","[Iteration] 20000 [cost] 0.004341908264905214\n","[Iteration] 30000 [cost] 0.003052770160138607\n","[Iteration] 40000 [cost] 0.002324355999007821\n","[Iteration] 50000 [cost] 0.0018568000523373485\n","[Iteration] 60000 [cost] 0.0015331348404288292\n","[Iteration] 70000 [cost] 0.001298172166571021\n","[Iteration] 80000 [cost] 0.0011220689630135894\n","[Iteration] 90000 [cost] 0.0009866665350273252\n","[Iteration] 100000 [cost] 0.0008798280614428222\n","[Iteration] 110000 [cost] 0.0007937114569358528\n","[Iteration] 120000 [cost] 0.00072256731800735\n","[Iteration] 130000 [cost] 0.0006632318836636841\n","[Iteration] 140000 [cost] 0.0006129226530902088\n","[Iteration] 150000 [cost] 0.0005692673730663955\n","[Iteration] 160000 [cost] 0.0005316268070600927\n","[Iteration] 170000 [cost] 0.0004988053697161376\n","[Iteration] 180000 [cost] 0.0004695183306466788\n","[Iteration] 190000 [cost] 0.00044313829857856035\n","tensor([[1.4365e-06],\n","        [3.6274e-05],\n","        [1.2059e-03],\n","        [9.9873e-01],\n","        [1.0000e+00],\n","        [1.0000e+00]], grad_fn=<SigmoidBackward0>)\n"]}],"source":["import torch\n","\n","x_train = torch.FloatTensor([[1, 2], [2, 3], [3, 4], [4, 4], [5, 3], [6, 2]])\n","y_train = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])\n","nH1 = 100\n","W_h = torch.randn([2, nH1], requires_grad = True) # 히든레이어의 W : W_h\n","b_h = torch.randn([nH1], requires_grad = True) # 히든 레이어의 b : b_h\n","W_o = torch.randn([nH1, 1], requires_grad = True) # 아웃풋 레이어의 W : W_o\n","b_o = torch.randn([1], requires_grad = True) # 아웃풋 레이어의 b : b_o\n","\n","optimizer = torch.optim.SGD([W_h, b_h, W_o, b_o], lr = 0.01)\n","\n","# [ 딥러닝 1단계 ] 모델을 만든다. Model setup\n","def H(x) :\n","  H1 = torch.sigmoid(torch.matmul(x, W_h) + b_h)\n","  model = torch.sigmoid(torch.matmul(H1, W_o) + b_o)\n","  return model\n","\n","# [ 딥러닝 2단계 ] 학습을 시킨다. Traning\n","for iter in range(200000):\n","  cost = torch.mean((-1)*y_train*torch.log(H(x_train))+(-1)*(1-y_train)*torch.log(1-H(x_train)))\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","  if iter % 10000 == 0 : print('[Iteration]', iter, '[cost]', cost.detach().item())\n","\n","# [ 딥러닝 3단계 ] 추론을 수행한다. Inference\n","\n","x_test = torch.FloatTensor([[7, 1]])\n","\n","model_result = H(x_train)\n","print(model_result)\n","\n","# print('Model Inference with [7,1] : ', model_result.detach().item())\n","# if model_result.detach().item() < 0.5:\n","#   print('F')\n","# else:\n","#   print('T')"]},{"cell_type":"code","source":["import torch\n","\n","x_train = torch.FloatTensor([[1, 2], [2, 3], [3, 4], [4, 4], [5, 3], [6, 2]])\n","y_train = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])\n","\n","nH1, nH2 = 100, 50\n","\n","W_h = torch.randn([2, nH1], requires_grad = True) # 히든레이어의 W : W_h\n","b_h = torch.randn([nH1], requires_grad = True) # 히든 레이어의 b : b_h\n","\n","W_h2 = torch.randn([nH1, nH2], requires_grad=True)\n","b_h2 = torch.randn([nH2], requires_grad=True)\n","\n","W_o = torch.randn([nH2, 1], requires_grad = True) # 아웃풋 레이어의 W : W_o\n","b_o = torch.randn([1], requires_grad = True) # 아웃풋 레이어의 b : b_o\n","\n","optimizer = torch.optim.SGD([W_h, b_h, W_h2, b_h2, W_o, b_o], lr=0.01)\n","\n","# [ 딥러닝 1단계 ] 모델을 만든다. Model setup\n","def H(x) :\n","  H1 = torch.sigmoid(torch.matmul(x, W_h) + b_h)\n","  H2 = torch.sigmoid(torch.matmul(H1, W_h2) + b_h2)\n","  model = torch.sigmoid(torch.matmul(H2, W_o) + b_o)\n","  return model\n","\n","# [ 딥러닝 2단계 ] 학습을 시킨다. Traning\n","for iter in range(200000):\n","  cost = torch.mean((-1)*y_train*torch.log(H(x_train))+(-1)*(1-y_train)*torch.log(1-H(x_train)))\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","  if iter % 10000 == 0 : print('[Iteration]', iter, '[cost]', cost.detach().item())\n","\n","# [ 딥러닝 3단계 ] 추론을 수행한다. Inference\n","\n","x_test = torch.FloatTensor([[7, 1]])\n","\n","model_result = H(x_train)\n","print(model_result)\n","\n","# print('Model Inference with [7,1] : ', model_result.detach().item())\n","# if model_result.detach().item() < 0.5:\n","#   print('F')\n","# else:\n","#   print('T')"],"metadata":{"id":"r3xndj12CAbi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730944567964,"user_tz":-540,"elapsed":213046,"user":{"displayName":"서영현","userId":"09271535808037779652"}},"outputId":"1c23b0c8-d977-4d26-d220-edc45aa0385c"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[Iteration] 0 [cost] 1.7320417165756226\n","[Iteration] 10000 [cost] 0.005265047308057547\n","[Iteration] 20000 [cost] 0.002740459283813834\n","[Iteration] 30000 [cost] 0.0018664011731743813\n","[Iteration] 40000 [cost] 0.001415030099451542\n","[Iteration] 50000 [cost] 0.0011374247260391712\n","[Iteration] 60000 [cost] 0.0009491873788647354\n","[Iteration] 70000 [cost] 0.0008133840747177601\n","[Iteration] 80000 [cost] 0.0007105885888449848\n","[Iteration] 90000 [cost] 0.0006300807581283152\n","[Iteration] 100000 [cost] 0.0005655457498505712\n","[Iteration] 110000 [cost] 0.0005125033785589039\n","[Iteration] 120000 [cost] 0.0004685240564867854\n","[Iteration] 130000 [cost] 0.0004309411160647869\n","[Iteration] 140000 [cost] 0.00039861988625489175\n","[Iteration] 150000 [cost] 0.0003711319004651159\n","[Iteration] 160000 [cost] 0.0003466573543846607\n","[Iteration] 170000 [cost] 0.00032496722997166216\n","[Iteration] 180000 [cost] 0.0003057231951970607\n","[Iteration] 190000 [cost] 0.00028829884831793606\n","tensor([[8.3988e-05],\n","        [1.1359e-04],\n","        [5.6747e-04],\n","        [9.9928e-01],\n","        [9.9993e-01],\n","        [9.9992e-01]], grad_fn=<SigmoidBackward0>)\n"]}]},{"cell_type":"code","source":["import torch\n","\n","x_train = torch.FloatTensor([[1, 2], [2, 3], [3, 4], [4, 4], [5, 3], [6, 2]])\n","y_train = torch.FloatTensor([[0], [0], [0], [1], [1], [1]])\n","\n","nH1, nH2, nH3 = 10, 5, 8\n","\n","W_h = torch.randn([2, nH1], requires_grad = True) # 히든레이어의 W : W_h\n","b_h = torch.randn([nH1], requires_grad = True) # 히든 레이어의 b : b_h\n","\n","W_h2 = torch.randn([nH1, nH2], requires_grad=True)\n","b_h2 = torch.randn([nH2], requires_grad=True)\n","\n","W_h3 = torch.randn([nH2, nH3], requires_grad=True)\n","b_h3 = torch.randn([nH3], requires_grad=True)\n","\n","W_o = torch.randn([nH3, 1], requires_grad = True) # 아웃풋 레이어의 W : W_o\n","b_o = torch.randn([1], requires_grad = True) # 아웃풋 레이어의 b : b_o\n","\n","optimizer = torch.optim.SGD([W_h, b_h, W_h2, b_h2, W_h3, b_h3, W_o, b_o], lr=0.01)\n","\n","# [ 딥러닝 1단계 ] 모델을 만든다. Model setup\n","def H(x) :\n","  H1 = torch.sigmoid(torch.matmul(x, W_h) + b_h)\n","  H2 = torch.sigmoid(torch.matmul(H1, W_h2) + b_h2)\n","  H3 = torch.sigmoid(torch.matmul(H2, W_h3) + b_h3)\n","  model = torch.sigmoid(torch.matmul(H3, W_o) + b_o)\n","  return model\n","\n","# [ 딥러닝 2단계 ] 학습을 시킨다. Traning\n","for iter in range(100000):\n","  cost = torch.mean((-1)*y_train*torch.log(H(x_train))+(-1)*(1-y_train)*torch.log(1-H(x_train)))\n","  optimizer.zero_grad()\n","  cost.backward()\n","  optimizer.step()\n","  if iter % 10000 == 0 : print('[Iteration]', iter, '[cost]', cost.detach().item())\n","\n","# [ 딥러닝 3단계 ] 추론을 수행한다. Inference\n","\n","x_test = torch.FloatTensor([[7, 1]])\n","\n","model_result = H(x_train)\n","print(model_result)\n","\n","# print('Model Inference with [7,1] : ', model_result.detach().item())\n","# if model_result.detach().item() < 0.5:\n","#   print('F')\n","# else:\n","#   print('T')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":483},"id":"yp2d5lKfTqjT","executionInfo":{"status":"error","timestamp":1730944632005,"user_tz":-540,"elapsed":64044,"user":{"displayName":"서영현","userId":"09271535808037779652"}},"outputId":"29e72962-dfad-4074-8687-4b78736867a2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[Iteration] 0 [cost] 1.463497519493103\n","[Iteration] 10000 [cost] 0.0057693966664373875\n","[Iteration] 20000 [cost] 0.0024149632081389427\n","[Iteration] 30000 [cost] 0.0014122944558039308\n","[Iteration] 40000 [cost] 0.000980695360340178\n","[Iteration] 50000 [cost] 0.0007446178351528943\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-9c1e8ddafce3>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m   \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Iteration]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'[cost]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"77xMJoM-TuhD"},"execution_count":null,"outputs":[]}]}