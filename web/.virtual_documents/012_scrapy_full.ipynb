


# install scrapy
#!pip install scrapy





!scrapy startproject news


!tree news








import scrapy, requests
from scrapy.http import TextResponse


# 링크 데이터


request = requests.get('https://news.daum.net')
response = TextResponse(request.url, body=request.text, encoding='utf-8')
response


selector = '/html/body/div[2]/main/section/div/div[1]/div[1]/ul/li'
selector += '/div/div/strong/a/@href'
links = response.xpath(selector).extract()
len(links), links[:2]


# 상세 데이터


link = links[19]
request = requests.get(link)
response = TextResponse(request.url, body=request.text, encoding='utf-8')
response


title = response.xpath('//*[@id="mArticle"]/div[1]/h3/text()')[0].extract()
content = response.xpath('//section//p/text()').extract()
content = ' '.join(content).replace('\xa0', ' ').replace("\'", ' ')
title, content[:100]





%%writefile news/news/items.py
import scrapy

class NewsContents(scrapy.Item):
    title = scrapy.Field()
    content = scrapy.Field()
    link = scrapy.Field()





%%writefile news/news/spiders/spider.py
import scrapy
from news.items import NewsContents


class NewsSpider(scrapy.Spider):
    name = 'news'
    allow_domain = ['daum.net']
    start_urls = ['https://news.daum.net']
    
    def parse(self, response):
        selector = '/html/body/div[2]/main/section/div/div[1]/div[1]/ul/li'
        selector += '/div/div/strong/a/@href'
        links = response.xpath(selector).extract()
        for link in links:
            yield scrapy.Request(link, callback=self.parse_content)
            
    def parse_content(self, response):
        item = NewsContents()
        item['title'] = response.xpath(
            '//*[@id="mArticle"]/div[1]/h3/text()')[0].extract()
        item['link'] = response.url
        content = response.xpath('//section//p/text()').extract()
        content = ' '.join(content).replace('\xa0', ' ').replace("\'", ' ')
        item['content'] = content
        yield item





import pandas as pd
pd.read_csv("news/news.csv")[['title', 'link', 'content']].tail(2)



